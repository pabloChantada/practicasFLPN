{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Embedding, Dense, Input, GlobalAveragePooling1D\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from dataset_reader import create_context, read_dataset, load_target_words\n",
    "from visualize import visualize_tsne_embeddings\n",
    "from cosine_sim import compute_cosine_similarities, save_cosine_similarities\n",
    "\n",
    "\n",
    "# Ruta del archivo .keras (ajusta la ruta según tu caso)\n",
    "modelo_path = \"/ruta/del/archivo/modelo.keras\"\n",
    "\n",
    "# Cargar el modelo\n",
    "modelo = tf.keras.models.load_model(modelo_path)\n",
    "\n",
    "# Ver la arquitectura del modelo cargado\n",
    "modelo.summary()\n",
    "@tf.keras.saving.register_keras_serializable()\n",
    "class WordModel(Model):\n",
    "    def __init__(self, vocab_size, embedding_size, window_size, **kwargs):\n",
    "        \"\"\"\n",
    "        Inicializa el modelo de predicción de palabras dado su contexto.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Tamaño del vocabulario.\n",
    "            embedding_size (int): Dimensión de los embeddings.\n",
    "            window_size (int): Tamaño de la ventana de contexto (número de palabras a cada lado).\n",
    "        \"\"\"\n",
    "        super(WordModel, self).__init__(**kwargs)\n",
    "\n",
    "        # Parámetros del modelo\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Capa de embedding para las palabras de contexto\n",
    "        self.embedding_context = Embedding(input_dim=vocab_size, \n",
    "                                           output_dim=embedding_size, \n",
    "                                           name=\"embedding_context\")\n",
    "\n",
    "        # Capa de promedio global para combinar los embeddings del contexto\n",
    "        self.average_layer = GlobalAveragePooling1D()\n",
    "\n",
    "        # Capa densa para predecir la palabra objetivo\n",
    "        self.output_layer = Dense(vocab_size, activation='softmax', name=\"output_layer\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Define el paso hacia adelante del modelo.\n",
    "\n",
    "        Args:\n",
    "            inputs (tensor): Ventana de contexto (batch_size, 2 * window_size).\n",
    "\n",
    "        Returns:\n",
    "            tensor: Predicción de la palabra objetivo (batch_size, vocab_size).\n",
    "        \"\"\"\n",
    "        # Obtener embeddings de las palabras de contexto\n",
    "        context_embedding = self.embedding_context(inputs)  # Forma: (batch_size, 2*window_size, embedding_size)\n",
    "\n",
    "        # Promediar los embeddings del contexto\n",
    "        averaged_embedding = self.average_layer(context_embedding)  # Forma: (batch_size, embedding_size)\n",
    "\n",
    "        # Predecir la palabra objetivo\n",
    "        output = self.output_layer(averaged_embedding)  # Forma: (batch_size, vocab_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        Método opcional para ver el modelo antes de entrenarlo.\n",
    "\n",
    "        Returns:\n",
    "            Model: Modelo de Keras con entradas y salidas definidas.\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=(2 * self.window_size,), name=\"input_context\")\n",
    "        return Model(inputs=inputs, outputs=self.call(inputs))\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Devuelve la configuración del modelo, incluyendo los parámetros necesarios.\n",
    "        \"\"\"\n",
    "        config = super(WordModel, self).get_config()  # Obtener configuración básica\n",
    "        config.update({\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embedding_size\": self.embedding_size,\n",
    "            \"window_size\": self.window_size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"\n",
    "        Reconstruir el modelo a partir de la configuración.\n",
    "        \"\"\"\n",
    "        return cls(\n",
    "            vocab_size=config[\"vocab_size\"],\n",
    "            embedding_size=config[\"embedding_size\"],\n",
    "            window_size=config[\"window_size\"],\n",
    "            **config\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "   \n",
    "    dataset_path = \"C:/Users/marce/Desktop/c/text8.txt\"\n",
    "    target_words_path = \"p2/materiales/target_words_text8.txt\"\n",
    "    model_name = \"embeddings/wordModel/word_embedding_text8.keras\"\n",
    "    plot_filename = \"plots/wordModel/tsne_embeddings_text8.png\"\n",
    "    cosine_filename = \"cosine/wordModel/cosine_similarities_text8.txt\"\n",
    "    window_size = 2  # Tamaño de la ventana de contexto (2 palabras antes y 2 después)\n",
    "    embedding_size = 300\n",
    "    batch_size = 1024\n",
    "    epochs = 20\n",
    "    \n",
    "\n",
    "    # Configuración común\n",
    "     # Dimensión de los embeddings\n",
    "\n",
    "    # 1. Leer el dataset y palabras objetivo\n",
    "    text = read_dataset(dataset_path)\n",
    "    target_words = load_target_words(target_words_path)\n",
    "\n",
    "    # 2. Tokenizar el texto\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index) + 1  # +1 para el token de padding\n",
    "\n",
    "    # Convertir el texto a secuencia de tokens\n",
    "    tokenized_text = tokenizer.texts_to_sequences([text])[0]\n",
    "    target_indexes = {word_index[word] for word in target_words if word in word_index}  # Convertir target a índices\n",
    "\n",
    "    # 3. Crear secuencias de entrenamiento usando ventana deslizante\n",
    "    X, y = create_context(tokenized_text, target_indexes, window_size)\n",
    "\n",
    "    # Mostrar información sobre los datos de entrenamiento\n",
    "    print(f\"Vocabulario: {len(word_index)} palabras únicas\")\n",
    "    print(f\"Número de secuencias de entrenamiento: {len(X)}\")\n",
    "    print(f\"Forma de X: {X.shape}\")\n",
    "    print(f\"Forma de y: {y.shape}\")\n",
    "\n",
    "    # Construir y compilar el modelo\n",
    "    model = WordModel(vocab_size, embedding_size, window_size)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Mostrar resumen del modelo\n",
    "    model.build_graph().summary()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    \n",
    "    history = model.fit(\n",
    "        X,  # Entrada: ventanas de contexto\n",
    "        y,  # Salida: palabras objetivo\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Guardar el modelo entrenado\n",
    "    model.save(model_name)\n",
    "\n",
    "    # Obtener los embeddings entrenados\n",
    "    embeddings = model.get_layer('embedding_context').get_weights()[0]\n",
    "\n",
    "    # Visualizar los embeddings de las palabras objetivo\n",
    "    visualize_tsne_embeddings(\n",
    "        words=target_words,  # Lista de palabras objetivo\n",
    "        embeddings=embeddings,  # Embeddings entrenados\n",
    "        word_index=word_index,  # Diccionario de palabras a índices\n",
    "        filename=plot_filename  # Guardar la visualización en un archivo\n",
    "    )\n",
    "\n",
    "    # Calcular similitudes de coseno\n",
    "    cosine_results = compute_cosine_similarities(target_words, word_index, embeddings)\n",
    "\n",
    "    # Imprimir similitudes de coseno\n",
    "    print(\"\\nSimilitudes de coseno:\")\n",
    "    for target_word, similar_words in cosine_results.items():\n",
    "        print(f\"Palabras más similares a '{target_word}':\")\n",
    "        for word, similarity in similar_words:\n",
    "            print(f\"  {word}: {similarity:.4f}\")\n",
    "        print()\n",
    "\n",
    "    # Guardar similitudes de coseno en un archivo de texto\n",
    " # Crear directorio si no existe\n",
    "    save_cosine_similarities(cosine_results, cosine_filename)\n",
    "    print(f\"Similitudes de coseno guardadas en '{cosine_filename}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
