{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_target        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_context       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_target    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │ input_target[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_context   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │ input_context[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_target… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_contex… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dot[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ reshape_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_target        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_context       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_target    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m)    │  \u001b[38;5;34m1,000,000\u001b[0m │ input_target[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_context   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m)    │  \u001b[38;5;34m1,000,000\u001b[0m │ input_context[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ embedding_target… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ embedding_contex… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (\u001b[38;5;33mDot\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_2 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dot[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ reshape_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,002</span> (7.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,000,002\u001b[0m (7.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,002</span> (7.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,000,002\u001b[0m (7.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Reshape, Dot, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Parámetros del modelo\n",
    "vocab_size = 10000  # Tamaño del vocabulario\n",
    "embedding_size = 100  # Dimensión de los embeddings\n",
    "context_window = 0  # Tamaño del contexto (2 palabras antes y 2 después)\n",
    "\n",
    "# Capa de entrada para la palabra objetivo\n",
    "input_target = Input(shape=(1,), name=\"input_target\")\n",
    "\n",
    "# Capa de entrada para la palabra de contexto\n",
    "input_context = Input(shape=(1,), name=\"input_context\")\n",
    "\n",
    "# Capa de embeddings para la palabra objetivo\n",
    "embedding_target = Embedding(input_dim=vocab_size, output_dim=embedding_size, name=\"embedding_target\")(input_target)\n",
    "embedding_target = Reshape((embedding_size, 1))(embedding_target)\n",
    "\n",
    "# Capa de embeddings para la palabra de contexto\n",
    "embedding_context = Embedding(input_dim=vocab_size, output_dim=embedding_size, name=\"embedding_context\")(input_context)\n",
    "embedding_context = Reshape((embedding_size, 1))(embedding_context)\n",
    "\n",
    "# Operación Dot para calcular la similitud entre los embeddings\n",
    "dot_product = Dot(axes=1)([embedding_target, embedding_context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "# Capa de salida con activación sigmoide\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "# Crear el modelo\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.preprocessing.text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.preprocessing.text'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "num_negative_samples = 0\n",
    "\n",
    "with open(\"datasets/game_of_thrones.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower().split()  \n",
    "\n",
    "# Cargar palabras de entrenamiento\n",
    "with open(\"materiales/target_words_game_of_thrones.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    palabras_entrenamiento = set(f.read().lower().split())\n",
    "\n",
    "# Tokenización del corpus\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts([\" \".join(text)])  \n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v: k for k, v in word_index.items()}  \n",
    "\n",
    "# Convertir texto en secuencia de índices\n",
    "sequence = [word_index[word] for word in text if word in word_index]\n",
    "\n",
    "# Listas para pares de entrenamiento\n",
    "pairs, labels = [], []\n",
    "\n",
    "# Generar pares positivos y negativos\n",
    "vocab_list = list(word_index.values())  # Lista de índices de palabras disponibles\n",
    "\n",
    "for i, word in enumerate(text):\n",
    "    if word in palabras_entrenamiento:  # Solo si la palabra está en la lista\n",
    "        window_start = max(i - context_window, 0)\n",
    "        window_end = min(i + context_window + 1, len(text))\n",
    "\n",
    "        context_words = []\n",
    "        for j in range(window_start, window_end):\n",
    "            if i != j:  # Evitar que la palabra se relacione consigo misma\n",
    "                pairs.append([word_index[word], word_index[text[j]]])\n",
    "                labels.append(1)  # Relación positiva\n",
    "                context_words.append(word_index[text[j]])\n",
    "\n",
    "        \n",
    "        for _ in range(num_negative_samples):\n",
    "            negative_word = random.choice(vocab_list)\n",
    "            while negative_word in context_words or negative_word == word_index[word]:  \n",
    "                negative_word = random.choice(vocab_list)  # Asegurar que no esté en el contexto real\n",
    "\n",
    "            pairs.append([word_index[word], negative_word])\n",
    "            labels.append(0)  # Relación negativa\n",
    "\n",
    "# Convertir a numpy arrays\n",
    "pairs = np.array(pairs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Guardar los pares en un archivo (opcional)\n",
    "np.savetxt(\"pares_entrenamiento_game_of_thrones.txt\", np.column_stack((pairs, labels)), fmt=\"%d\")\n",
    "\n",
    "# Mostrar ejemplos\n",
    "for i in range(10):\n",
    "    target, context = pairs[i]\n",
    "    print(f\"Entrada: [{index_word[target]}, {index_word[context]}] -> Salida: {labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"datasets/game_of_thrones.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower().split()  \n",
    "\n",
    "\n",
    "unique_words = set(text)\n",
    "\n",
    "\n",
    "print(f\"El corpus tiene {len(unique_words)} palabras distintas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = np.loadtxt(\"pares_entrenamiento_game_of_thrones.txt\", dtype=int)\n",
    "\n",
    "\n",
    "targets, contexts, labels = data[:, 0], data[:, 1], data[:, 2]\n",
    "\n",
    "\n",
    "targets = tf.convert_to_tensor(targets, dtype=tf.int32)\n",
    "contexts = tf.convert_to_tensor(contexts, dtype=tf.int32)\n",
    "labels = tf.convert_to_tensor(labels, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "model.fit([targets, contexts], labels, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "embeddings = model.get_layer(\"embedding_target\").get_weights()[0]\n",
    "\n",
    "\n",
    "np.save(\"word_embeddings_game_of_thrones.npy\", embeddings)\n",
    "\n",
    "\n",
    "import json\n",
    "with open(\"word_index_game_of_thrones.json\", \"w\") as f:\n",
    "    json.dump(tokenizer.word_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_tsne_embeddings(words, embeddings, word_index, filename=None):\n",
    "    \"\"\"\n",
    "    Visualizes t-SNE embeddings of selected words.\n",
    "\n",
    "    Args:\n",
    "        words (list): List of words to visualize.\n",
    "        embeddings (numpy.ndarray): Array containing word embeddings.\n",
    "        word_index (dict): Mapping of words to their indices in the embeddings array.\n",
    "        filename (str, optional): File to save the visualization. If None, plot is displayed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Filter the embeddings for the selected words\n",
    "    indices = [word_index[word] for word in words]\n",
    "    selected_embeddings = embeddings[indices]\n",
    "\n",
    "    # Set perplexity for t-SNE, it's recommended to use a value less than the number of selected words\n",
    "    perplexity = min(5,len(words) - 1)\n",
    "\n",
    "    # Use t-SNE to reduce dimensionality\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=0)\n",
    "    reduced_embeddings = tsne.fit_transform(selected_embeddings)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, word in enumerate(words):\n",
    "        plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1])\n",
    "        plt.annotate(word, xy=(reduced_embeddings[i, 0], reduced_embeddings[i, 1]), xytext=(5, 2),\n",
    "                     textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "    # Save or display the plot\n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = np.load(\"word_embeddings_game_of_thrones.npy\")\n",
    "with open(\"word_index_game_of_thrones.json\", \"r\") as f:\n",
    "    word_index = json.load(f)\n",
    "\n",
    "\n",
    "with open(\"materiales/target_words_game_of_thrones.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    words_to_visualize = f.read().lower().split()\n",
    "\n",
    "\n",
    "visualize_tsne_embeddings(words_to_visualize, embeddings, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harry potter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    \"\"\"\n",
    "    Resetea los pesos del modelo sin cambiar su arquitectura.\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.assign(layer.kernel_initializer(tf.keras.backend.shape(layer.kernel)))\n",
    "        if hasattr(layer, 'bias_initializer') and layer.bias is not None:\n",
    "            layer.bias.assign(layer.bias_initializer(tf.keras.backend.shape(layer.bias)))\n",
    "\n",
    "\n",
    "reset_weights(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "num_negative_samples = 0\n",
    "\n",
    "with open(\"datasets/harry_potter_and_the_philosophers_stone.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower().split()  \n",
    "\n",
    "\n",
    "with open(\"materiales/target_words_harry_potter.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    palabras_entrenamiento = set(f.read().lower().split())\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts([\" \".join(text)])  \n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v: k for k, v in word_index.items()}  \n",
    "\n",
    "\n",
    "sequence = [word_index[word] for word in text if word in word_index]\n",
    "\n",
    "\n",
    "pairs, labels = [], []\n",
    "\n",
    "\n",
    "vocab_list = list(word_index.values())  \n",
    "\n",
    "for i, word in enumerate(text):\n",
    "    if word in palabras_entrenamiento:  \n",
    "        window_start = max(i - context_window, 0)\n",
    "        window_end = min(i + context_window + 1, len(text))\n",
    "\n",
    "        context_words = []\n",
    "        for j in range(window_start, window_end):\n",
    "            if i != j:  \n",
    "                pairs.append([word_index[word], word_index[text[j]]])\n",
    "                labels.append(1)  \n",
    "                context_words.append(word_index[text[j]])\n",
    "\n",
    "       \n",
    "        for _ in range(num_negative_samples):\n",
    "            negative_word = random.choice(vocab_list)\n",
    "            while negative_word in context_words or negative_word == word_index[word]:  \n",
    "                negative_word = random.choice(vocab_list)  \n",
    "\n",
    "            pairs.append([word_index[word], negative_word])\n",
    "            labels.append(0)  \n",
    "\n",
    "\n",
    "pairs = np.array(pairs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "np.savetxt(\"pares_entrenamiento_harry_potter.txt\", np.column_stack((pairs, labels)), fmt=\"%d\")\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    target, context = pairs[i]\n",
    "    print(f\"Entrada: [{index_word[target]}, {index_word[context]}] -> Salida: {labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "unique_words = set(text)\n",
    "\n",
    "\n",
    "print(f\"El corpus tiene {len(unique_words)} palabras distintas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"pares_entrenamiento_harry_potter.txt\", dtype=int)\n",
    "\n",
    "targets, contexts, labels = data[:, 0], data[:, 1], data[:, 2]\n",
    "\n",
    "\n",
    "targets = tf.convert_to_tensor(targets, dtype=tf.int32)\n",
    "contexts = tf.convert_to_tensor(contexts, dtype=tf.int32)\n",
    "labels = tf.convert_to_tensor(labels, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "model.fit([targets, contexts], labels, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los pesos de la capa de embedding\n",
    "embeddings = model.get_layer(\"embedding_target\").get_weights()[0]\n",
    "\n",
    "# Guardar los embeddings en un archivo\n",
    "np.save(\"word_embeddings_harry_potter.npy\", embeddings)\n",
    "\n",
    "# Guardar el índice de palabras para futura referencia\n",
    "import json\n",
    "with open(\"word_index_harry_potter.json\", \"w\") as f:\n",
    "    json.dump(tokenizer.word_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = np.load(\"word_embeddings_harry_potter.npy\")\n",
    "with open(\"word_index_harry_potter.json\", \"r\") as f:\n",
    "    word_index = json.load(f)\n",
    "\n",
    "\n",
    "with open(\"materiales/target_words_harry_potter.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    words_to_visualize = f.read().lower().split()\n",
    "\n",
    "\n",
    "visualize_tsne_embeddings(words_to_visualize, embeddings, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "reset_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "num_negative_samples = 0\n",
    "\n",
    "with open(\"datasets/the_fellowship_of_the_ring.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower().split()  \n",
    "\n",
    "\n",
    "with open(\"materiales/target_words_the_fellowship_of_the_ring.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    palabras_entrenamiento = set(f.read().lower().split())\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts([\" \".join(text)])  \n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v: k for k, v in word_index.items()}  \n",
    "\n",
    "\n",
    "sequence = [word_index[word] for word in text if word in word_index]\n",
    "\n",
    "\n",
    "pairs, labels = [], []\n",
    "\n",
    "\n",
    "vocab_list = list(word_index.values())  \n",
    "\n",
    "for i, word in enumerate(text):\n",
    "    if word in palabras_entrenamiento:  \n",
    "        window_start = max(i - context_window, 0)\n",
    "        window_end = min(i + context_window + 1, len(text))\n",
    "\n",
    "        context_words = []\n",
    "        for j in range(window_start, window_end):\n",
    "            if i != j:  \n",
    "                pairs.append([word_index[word], word_index[text[j]]])\n",
    "                labels.append(1)  \n",
    "                context_words.append(word_index[text[j]])\n",
    "\n",
    "       \n",
    "        for _ in range(num_negative_samples):\n",
    "            negative_word = random.choice(vocab_list)\n",
    "            while negative_word in context_words or negative_word == word_index[word]:  \n",
    "                negative_word = random.choice(vocab_list)  \n",
    "\n",
    "            pairs.append([word_index[word], negative_word])\n",
    "            labels.append(0)  \n",
    "\n",
    "\n",
    "pairs = np.array(pairs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "np.savetxt(\"pares_entrenamiento_fellowship.txt\", np.column_stack((pairs, labels)), fmt=\"%d\")\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    target, context = pairs[i]\n",
    "    print(f\"Entrada: [{index_word[target]}, {index_word[context]}] -> Salida: {labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "unique_words = set(text)\n",
    "\n",
    "\n",
    "print(f\"El corpus tiene {len(unique_words)} palabras distintas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"pares_entrenamiento_fellowship.txt\", dtype=int)\n",
    "\n",
    "targets, contexts, labels = data[:, 0], data[:, 1], data[:, 2]\n",
    "\n",
    "\n",
    "targets = tf.convert_to_tensor(targets, dtype=tf.int32)\n",
    "contexts = tf.convert_to_tensor(contexts, dtype=tf.int32)\n",
    "labels = tf.convert_to_tensor(labels, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "model.fit([targets, contexts], labels, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los pesos de la capa de embedding\n",
    "embeddings = model.get_layer(\"embedding_target\").get_weights()[0]\n",
    "\n",
    "# Guardar los embeddings en un archivo\n",
    "np.save(\"word_embeddings_fellowship.npy\", embeddings)\n",
    "\n",
    "# Guardar el índice de palabras para futura referencia\n",
    "import json\n",
    "with open(\"word_index_fellowship.json\", \"w\") as f:\n",
    "    json.dump(tokenizer.word_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = np.load(\"word_embeddings_fellowship.npy\")\n",
    "with open(\"word_index_fellowship.json\", \"r\") as f:\n",
    "    word_index = json.load(f)\n",
    "\n",
    "\n",
    "with open(\"materiales/target_words_the_fellowship_of_the_ring.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    words_to_visualize = f.read().lower().split()\n",
    "\n",
    "\n",
    "visualize_tsne_embeddings(words_to_visualize, embeddings, word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
