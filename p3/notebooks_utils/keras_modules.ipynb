{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNcLEKQVLsuDXa5HdMaXbee"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Some relevant Tensorflow and Keras modules that we will be using for our LSTM model:**\n","\n","- https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n","- https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding \n","- https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n","- https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten\n","- https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n","- https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical\n","- https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n","- https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed\n","- https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n"],"metadata":{"id":"ymx3lVwVFerM"}},{"cell_type":"markdown","source":["**Keras modules required to train the feed-forward model**\n"],"metadata":{"id":"BnmhKOQLR_vH"}},{"cell_type":"code","source":["\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, LSTM\n","import tensorflow as tf\n","\n","dummy_input_size = 5\n","dummy_vocab_size = 100\n","dummy_emb_size=20\n","dummy_nlabels=17\n","dummy_hidden_dim=128\n","\n","model = Sequential()\n","model.add(Input(shape=(dummy_input_size,),dtype=tf.int32)) \n","\n","model.add(Embedding(input_dim=dummy_vocab_size, output_dim=dummy_emb_size,\n","                    mask_zero=True, \n","                    input_length=dummy_input_size))\n","\n","model.add(Flatten())\n","\n","model.add(Dense(dummy_hidden_dim, activation='relu'))\n","model.add(Dense(dummy_nlabels, activation='softmax'))\n","\n","model.summary()"],"metadata":{"id":"zEXFkK1SR-7i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677056400897,"user_tz":-60,"elapsed":514,"user":{"displayName":"David V","userId":"14796473427498895857"}},"outputId":"84a66393-abd1-4a63-968d-26c1ac91124b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 5, 20)             2000      \n","                                                                 \n"," flatten (Flatten)           (None, 100)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 128)               12928     \n","                                                                 \n"," dense_2 (Dense)             (None, 17)                2193      \n","                                                                 \n","=================================================================\n","Total params: 17,121\n","Trainable params: 17,121\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["**Keras modules required to create our LSTM model**\n","\n","\n"],"metadata":{"id":"a9MYjOxXIImO"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yIZghlwaD3pL","executionInfo":{"status":"ok","timestamp":1677056720539,"user_tz":-60,"elapsed":1433,"user":{"displayName":"David V","userId":"14796473427498895857"}},"outputId":"9ed3c659-cdbd-4d55-d1e4-4886788b53cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_7 (Embedding)     (None, 64, 20)            2000      \n","                                                                 \n"," lstm_6 (LSTM)               (None, 64, 128)           76288     \n","                                                                 \n"," time_distributed_5 (TimeDis  (None, 64, 50)           6450      \n"," tributed)                                                       \n","                                                                 \n","=================================================================\n","Total params: 84,738\n","Trainable params: 84,738\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Embedding, Dense, LSTM, TimeDistributed\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","dummy_num_words = 100\n","dummy_nlabels = 50\n","dummy_emb_dim = 20\n","dummy_max_seq_length = 64\n","lstm_out_dim = 128\n","\n","model = Sequential()\n","model.add(Embedding(dummy_num_words, dummy_emb_dim, input_length=dummy_max_seq_length, mask_zero=True))\n","model.add(LSTM(lstm_out_dim, return_sequences=True))\n","model.add(TimeDistributed(Dense(dummy_nlabels, activation='softmax')))\n","\n","model.summary()\n","\n"]},{"cell_type":"markdown","source":["**We can save the model after training is finished** (ignore the WARNING for this specific example)"],"metadata":{"id":"mKbvEr48Puet"}},{"cell_type":"code","source":["model.save(\"dummy_model.h5\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWQnZ-lMPxyV","executionInfo":{"status":"ok","timestamp":1677056750228,"user_tz":-60,"elapsed":230,"user":{"displayName":"David V","userId":"14796473427498895857"}},"outputId":"b3ef31e4-ed82-4043-e258-4a222934e3de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}]},{"cell_type":"markdown","source":["**We also can load the model to use it anytime later**"],"metadata":{"id":"5_fpp3lbRyxH"}},{"cell_type":"code","source":["import tensorflow\n","model = tensorflow.keras.models.load_model(\"dummy_model.h5\")\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_wStL7xSQ2O","executionInfo":{"status":"ok","timestamp":1677056758678,"user_tz":-60,"elapsed":2098,"user":{"displayName":"David V","userId":"14796473427498895857"}},"outputId":"35f310b4-6b8d-43a1-f2d7-bc30bd513bff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"sequential_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_7 (Embedding)     (None, 64, 20)            2000      \n","                                                                 \n"," lstm_6 (LSTM)               (None, 64, 128)           76288     \n","                                                                 \n"," time_distributed_5 (TimeDis  (None, 64, 50)           6450      \n"," tributed)                                                       \n","                                                                 \n","=================================================================\n","Total params: 84,738\n","Trainable params: 84,738\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["**pad_sequences: pads contexts that have less than max_seq_length words. For instance:**"],"metadata":{"id":"DsAMaShdIqd1"}},{"cell_type":"code","source":["sequences = [[],[1],[3,2],[4,5,7,8]]\n","pad_sequences(sequences)"],"metadata":{"id":"w2PwevctIP5U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677056763454,"user_tz":-60,"elapsed":185,"user":{"displayName":"David V","userId":"14796473427498895857"}},"outputId":"0df52499-423a-479e-aa69-41a1c7e8d6e3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 0, 0],\n","       [0, 0, 0, 1],\n","       [0, 0, 3, 2],\n","       [4, 5, 7, 8]], dtype=int32)"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["**to_categorical: transforms a set of discrete classes into their one-hot representation**\n"],"metadata":{"id":"oOaYVm_jGgPW"}},{"cell_type":"code","source":["import tensorflow\n","discrete_classes = [0, 1, 2, 3, 0, 4, 5, 3, 3, 3]\n","print (\"Discrete classes:\", discrete_classes)\n","categorical_classes = tensorflow.keras.utils.to_categorical(discrete_classes, num_classes=len(set(discrete_classes)))\n","print (\"Categorical classes:\")\n","print (categorical_classes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q5eXyFwmGfvg","executionInfo":{"status":"ok","timestamp":1677055217551,"user_tz":-60,"elapsed":9,"user":{"displayName":"David V","userId":"14796473427498895857"}},"outputId":"37d1ea25-5ce2-4bbd-a2cb-551a307be9ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Discrete classes: [0, 1, 2, 3, 0, 4, 5, 3, 3, 3]\n","Categorical classes:\n","[[1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["**Create a set of samples using [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) and use it to train any keras model**"],"metadata":{"id":"sEpuDLRCpf8D"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","#Dummy training data, three possible inputs and their corresponding outputs\n","X = [[0,0,1],[0,1,0],[1,0,0]]\n","y = [0,0,1]\n","\n","weights = [0.5,0.5, 1.25]\n","\n","dummy_batch_size=1\n","use_weights=False\n","\n","if use_weights:\n","  dataset = tf.data.Dataset.from_tensor_slices((X, y, weights))\n","  dataset = dataset.batch(dummy_batch_size)\n","else:\n","  dataset = tf.data.Dataset.from_tensor_slices((X, y))\n","  dataset = dataset.batch(dummy_batch_size)\n","\n","# Define the logistic regression model\n","model = Sequential()\n","model.add(Dense(1, input_dim=3, activation='sigmoid'))\n","\n","# Compile the model with binary crossentropy loss and Adam optimizer\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], weighted_metrics=[])\n","\n","# Train the model on the input and output data\n","model.fit(dataset, epochs=1, verbose=1)\n","\n","# Evaluate the model on the same input data (here we do no respect the train/dev methodology, but it's just for illustrative purposes)\n","loss, accuracy = model.evaluate(dataset, verbose=0)\n","\n","print (\"Final loss/accuracy\", loss, accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Co6Ayq9wpxRz","executionInfo":{"status":"ok","timestamp":1677061989377,"user_tz":-60,"elapsed":517,"user":{"displayName":"David V","userId":"14796473427498895857"}},"outputId":"a07047f4-c5ae-44b6-d102-4f17fd918d18"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["3/3 [==============================] - 0s 5ms/step - loss: 0.6434 - accuracy: 0.6667\n","Final loss/accuracy 0.6423259377479553 0.6666666865348816\n"]}]}]}